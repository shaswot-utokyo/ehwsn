{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "import random\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "\n",
    "import gym\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.optim as optim\n",
    "\n",
    "cmap = plt.get_cmap(\"tab20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# set seed\n",
    "seed = 123\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from common import experiments_koi as experiments\n",
    "from common import common_fn, env_lib, wrappers\n",
    "from common.dqn_model import DQN, DuelingDQN, TargetNet\n",
    "from common.common_fn import Experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT:  koi-eno-g99\n"
     ]
    }
   ],
   "source": [
    "experiment = \"koi-eno-g99\"#args.experiment\n",
    "params = experiments.EXP_PARAMS[experiment]\n",
    "print(\"EXPERIMENT: \", experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def wrap_all(env):\n",
    "    env = wrappers.symmetric_normalize_obs(env)\n",
    "#     env = wrappers.high_negative_reward(env)\n",
    "#     env = wrappers.sparse_reward_v3(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "# Create environment\n",
    "env = eval(\"env_lib.\"+params['env_name']+'()')\n",
    "env = wrap_all(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE # cuda :  Tesla V100-PCIE-32GB\n"
     ]
    }
   ],
   "source": [
    "# evaluate Q-values of random states\n",
    "NO_OF_STATES_TO_EVALUATE = 1000 # how many states to sample to evaluate\n",
    "EVAL_FREQ = 240 # how often to evaluate\n",
    "\n",
    "# n-step rollout\n",
    "ROLLOUT_STEPS = params['rollout_steps']\n",
    "\n",
    "# Width of NN\n",
    "NN_LAYER_WIDTH = params['nn_layer_width']\n",
    "\n",
    "# Double DQN flag\n",
    "DOUBLE = params['double_q']\n",
    "\n",
    "# Dueling Networks\n",
    "DUELING = params['dueling']\n",
    "\n",
    "# set device\n",
    "USE_GPU = True#args.cuda\n",
    "USE_CUDA = torch.cuda.is_available() and USE_GPU\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "if USE_CUDA:\n",
    "    print(\"DEVICE #\", device, \": \", torch.cuda.get_device_name(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAG:  koi-eno-g99-1step-123-18_Mar-16_33_37\n"
     ]
    }
   ],
   "source": [
    "tag = experiment\n",
    "nstep_tag = str(params['rollout_steps'])+\"step\"\n",
    "tag = experiment + '-' + nstep_tag  # nstep rollouts\n",
    "# if DOUBLE:\n",
    "#     double_tag = 'double'\n",
    "#     tag = tag + '-' + double_tag # double dqn\n",
    "\n",
    "# if DUELING:\n",
    "#     dueling_tag = 'dueling'\n",
    "#     tag = tag + '-' + dueling_tag # double dqn\n",
    "\n",
    "tag = tag + '-' + str(seed) # seed\n",
    "tag = tag + '-' + datetime.datetime.now().strftime(\"%d_%b-%H_%M_%S\")\n",
    "print(\"TAG: \",tag)\n",
    "writer_folder = './runs/'+ experiment + \"/\" + tag\n",
    "writer = SummaryWriter(log_dir=writer_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if DUELING:\n",
    "    policy_net = DuelingDQN(env.observation_space.shape[0],\n",
    "                               env.action_space.n,\n",
    "                               NN_LAYER_WIDTH).to(device)\n",
    "    \n",
    "else:\n",
    "    policy_net = DQN(env.observation_space.shape[0], \n",
    "                        env.action_space.n,\n",
    "                        NN_LAYER_WIDTH).to(device)\n",
    "\n",
    "    \n",
    "target_net = TargetNet(policy_net)\n",
    "target_net.sync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "buffer = common_fn.nstep_ExperienceBuffer(params['replay_size'],\n",
    "                                nsteps=params['rollout_steps'],\n",
    "                                gamma=params['gamma'])\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=params['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ITERATION:  1 / 5\n",
      "DAY:  338\r"
     ]
    }
   ],
   "source": [
    "train_location = 'tokyo'\n",
    "train_year = 1995\n",
    "epsilon = params['epsilon_start']\n",
    "frame_idx = 0\n",
    "NO_OF_ITERATIONS = 5\n",
    "\n",
    "for iteration in range(NO_OF_ITERATIONS):\n",
    "    start_time = datetime.datetime.now()\n",
    "    print(\"\\n\\nITERATION: \",iteration+1,\"/\",NO_OF_ITERATIONS)\n",
    "    \n",
    "    # FOR EACH ITERATION\n",
    "    eval_states = None # will be populated with held-out states\n",
    "\n",
    "    reward_rec = []\n",
    "    RECOVERY_rec = []\n",
    "\n",
    "    state = env.reset(location=train_location, \n",
    "                      year=train_year,\n",
    "                      LOG_DATA=True)\n",
    "    is_done = False\n",
    "    present_day = -1\n",
    "    while not is_done:\n",
    "        # Check if day has changed\n",
    "        if env.env_harvester.day != present_day:\n",
    "            print(\"DAY: \", env.env_harvester.day, end='\\r')\n",
    "            present_day = env.env_harvester.day\n",
    "        \n",
    "        # Increase frame count\n",
    "        frame_idx += 1\n",
    "        \n",
    "        # Log RECOVERY MODE\n",
    "        RECOVERY_rec.append(env.RECOVERY_MODE)\n",
    "        \n",
    "        # No action/training if in RECOVERY MODE\n",
    "        if env.RECOVERY_MODE:\n",
    "            _, reward, is_done, _ = env.step(-1)        \n",
    "\n",
    "        else:\n",
    "            # Get epsilon\n",
    "            epsilon = max(params['epsilon_final'], params['epsilon_start'] - frame_idx / params['epsilon_frames'])\n",
    "            writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
    "\n",
    "            # Get action\n",
    "            if np.random.random() < epsilon: # random exploratory action\n",
    "                action = np.random.randint(env.action_space.n)\n",
    "            else: # greedy action\n",
    "                with torch.no_grad():\n",
    "                    state_v = torch.tensor(np.array(state, copy=False)).unsqueeze(dim=0).to(device) # convert to torch tensor\n",
    "                    q_vals_v = policy_net(state_v) # get Q-values\n",
    "                    _, action_idx = torch.max(q_vals_v, dim=1) # Argmax Q-values and extract action\n",
    "                    action = int(action_idx.item()) # convert from torch to python variable\n",
    "\n",
    "            # do step in the environment\n",
    "            new_state, reward, is_done, _ = env.step(action)\n",
    "            \n",
    "            # Record experience\n",
    "            exp = Experience(state, action, reward, is_done, new_state)\n",
    "            buffer.append(exp)\n",
    "            state = new_state\n",
    "\n",
    "        # Record reward\n",
    "        reward_rec.append(reward)\n",
    "#             mean_reward = np.mean(reward_rec[-240:]) # mean of last 100 episodes\n",
    "#             writer.add_scalar(\"Reward/reward_day\", mean_reward, frame_idx)\n",
    "        writer.add_scalar(\"Reward/reward\", reward, frame_idx)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        # Is buffer filled enough to start training?\n",
    "        if len(buffer) < params['replay_initial']:\n",
    "            continue \n",
    "        # Train\n",
    "        optimizer.zero_grad()\n",
    "        batch = buffer.sample(params['batch_size'])\n",
    "        loss_v = common_fn.calc_loss(batch, \n",
    "                                      policy_net, \n",
    "                                      target_net.target_model, \n",
    "                                      gamma=params['gamma']**ROLLOUT_STEPS,\n",
    "                                      device=device,\n",
    "                                      double=DOUBLE)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Record loss\n",
    "        if frame_idx % 1E3 == 0:\n",
    "            writer.add_scalar(\"loss\", loss_v, frame_idx)\n",
    "\n",
    "        # Sync policy_net and target_net\n",
    "        if frame_idx % params['target_net_sync'] == 0:\n",
    "            target_net.sync()\n",
    "        \n",
    "        # evaluate states\n",
    "        if frame_idx % EVAL_FREQ == 0:\n",
    "            eval_states = buffer.sample(1000)[0]\n",
    "            mean_val = common_fn.calc_values_of_states(eval_states, policy_net, device=device)\n",
    "            writer.add_scalar(\"values_mean\", mean_val, frame_idx)\n",
    "\n",
    "        # env ends\n",
    "        ########################################################\n",
    "    \n",
    "    common_fn.display_env_log_eno(env,\n",
    "                reward_rec, \n",
    "                RECOVERY_rec,\n",
    "                START_DAY=0, \n",
    "                NO_OF_DAY_TO_PLOT = 400)\n",
    "    common_fn.display_env_log_eno(env,\n",
    "                reward_rec, \n",
    "                RECOVERY_rec,\n",
    "                START_DAY=40, \n",
    "                NO_OF_DAY_TO_PLOT = 10)\n",
    "    \n",
    "    print(\"AVG REWARD:\\t\", np.mean(reward_rec))\n",
    "    print(\"ENERGY PERF:\\t\", np.array(env.eno_log).cumsum()[-1])\n",
    "    print(\"RECOVERY(DAYS):\\t\", np.count_nonzero(RECOVERY_rec)/len(env.env_timeslot_values))\n",
    "    print(\"TIME ELAPSED:\\t\",  datetime.datetime.now()  - start_time)\n",
    "    # training iteration ends\n",
    "    ########################################################\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Save agent\n",
    "import os.path\n",
    "cur_folder = os.getcwd()\n",
    "model_folder = os.path.join(cur_folder,\"models\")\n",
    "if not os.path.exists(model_folder):\n",
    "    os.makedirs(model_folder)\n",
    "\n",
    "model_file = os.path.join(model_folder, (tag + \".pt\"))\n",
    "torch.save(policy_net.state_dict(), model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Enjoy trained agent\n",
    "test_location = 'tokyo'\n",
    "test_year = 2000\n",
    "for test_year in [1995]:\n",
    "    print(\"\\n\\nTEST YEAR:\", test_year)\n",
    "    test_env = eval(\"env_lib.\"+params['env_name']+'()')\n",
    "    test_env = wrap_all(test_env)\n",
    "\n",
    "    test_reward_rec = []\n",
    "    test_RECOVERY_rec = []\n",
    "\n",
    "    obs = test_env.reset(location=test_location,\n",
    "                    year=test_year,\n",
    "                    LOG_DATA=True)\n",
    "\n",
    "    DAYS_TO_ITERATE = 400\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        state_v = torch.tensor(np.array(obs, copy=False)).unsqueeze(dim=0)\n",
    "        q_vals = policy_net(state_v.to(device)).data.cpu().numpy()[0]\n",
    "        action = np.argmax(q_vals)\n",
    "        obs, reward, done, info = test_env.step(action)\n",
    "        test_reward_rec.append(reward)\n",
    "        test_RECOVERY_rec.append(test_env.RECOVERY_MODE)\n",
    "\n",
    "    common_fn.display_env_log_eno(test_env,\n",
    "                    test_reward_rec, \n",
    "                    test_RECOVERY_rec,\n",
    "                    START_DAY=0, \n",
    "                    NO_OF_DAY_TO_PLOT = 500)\n",
    "    \n",
    "    print(\"AVG REWARD:\\t\", np.mean(reward_rec))\n",
    "    print(\"ENERGY PERF:\\t\", np.array(env.eno_log).cumsum()[-1])\n",
    "    print(\"RECOVERY(DAYS):\\t\", np.count_nonzero(RECOVERY_rec)/len(env.env_timeslot_values))\n",
    "\n",
    "    actions_rec = np.floor(np.array(test_env.action_log)*10+1)\n",
    "    plt.hist(actions_rec,bins=np.arange(test_env.action_space.n))\n",
    "    plt.yscale(\"log\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "common_fn.display_env_log_eno(test_env,\n",
    "                test_reward_rec, \n",
    "                test_RECOVERY_rec,\n",
    "                START_DAY=40, \n",
    "                NO_OF_DAY_TO_PLOT = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
